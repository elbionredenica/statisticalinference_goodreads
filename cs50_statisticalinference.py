# -*- coding: utf-8 -*-
"""cs50_statisticalinference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CxEyNRKzVHFkm1v3KbWR7_27RQxtPVBG
"""

# import packages and libraries
import pandas as pd
import numpy as np
from scipy import stats
import matplotlib.pyplot as plt

# import data 

# reading csv from url
df = pd.read_csv("https://docs.google.com/spreadsheets/d/1if5bG_Nld7iQC3FcUBIv2SLT12HQSCqvawvN67iZ-yE/gviz/tq?tqx=out:csv")

# shows the first 6 titles from the dataset
df.head(6)

# printing summary statistics: count, mean, standard deviation* (calculated with Bessel's correction**), range, etc.
df[['average_rating','num_pages']].describe()

# * pandas documentation: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.std.html
# ** Bessel's correction is a correction that is applied to the sample variance in order to make it an unbiased estimator of the population variance. 
#    The sample variance is calculated by taking the sum of the squared differences between each data point and the sample mean, and then dividing 
#    by the number of data points minus one. This is a biased estimator because it underestimates the population variance. Bessel's correction is 
#    applied by dividing by the number of data points minus one, rather than just the number of data points. This corrects the bias and gives an 
#    unbiased estimate of the population variance.

# other statistics ".describe()" does not include--median & mode
# the function calculates the median and mode using libraries function
import numpy as np
from scipy import stats 

def additionalStats(sample):
  print("Median:", np.median(sample))
  print("Mode:", stats.mode(sample)[0][0])

print("Average Book Rating additional stats")
additionalStats(df.average_rating)
print("\n")
print("Book Pages additional stats")
additionalStats(df.num_pages)

# #dataviz: create visualization for each group
# use this cell to create your histograms
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

df = pd.read_csv("https://docs.google.com/spreadsheets/d/1if5bG_Nld7iQC3FcUBIv2SLT12HQSCqvawvN67iZ-yE/gviz/tq?tqx=out:csv")

dataAboveThreshold = df[df['num_pages'] >= 350]
dataUnderThreshold = df[df['num_pages'] < 350]

# # create histogram for each variable

# created reasonable bins for a better observation of the average rating histogram
bins = [0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5]

# average ratings with all books 
# df.average_rating
# using numpy's np.array() to create an array and plot it in a histogram
averageRatingsAllData = np.array(df.average_rating)
plt.hist(averageRatingsAllData, bins, facecolor='r', alpha=0.7, edgecolor='k', linewidth=0)
plt.title("Average book ratings distribution from data of all books")
plt.xlabel("Average Rating")
plt.ylabel("Frequency")
# displaying all bins 
plt.xticks(bins[::1]) 
# rotating ticks as they would overlap
plt.xticks(rotation = 45)
plt.show()

# num of pages of all books 
# df.num_pages
# using numpy's np.array() to create an array and plot it in a histogram
numPagesAllData = np.array(df.num_pages)
plt.hist(numPagesAllData, facecolor='r', alpha=0.7, edgecolor='k', linewidth=0)
plt.title("Book length distribution from data of all books")
plt.xlabel("Number of book pages")
plt.ylabel("Frequency")
plt.show()


# # the second graph
# # average ratings with books >= 350
# # dataAboveThreshold.average_rating, dataAboveThreshold.num_pages
# averageRatingsAboveThreshold = np.array(dataAboveThreshold.average_rating)
# plt.hist(averageRatingsAboveThreshold, bins, facecolor='r', alpha=0.7, edgecolor='k', linewidth=1)
# plt.title("Average book ratings distribution from data of books containing 500 or more pages")
# plt.xlabel("Average Rating")
# plt.ylabel("Frequency")
# plt.xticks(bins[::1]) 
# plt.xticks(rotation = 45)
# plt.show()

# # the third graph
# # average ratings with with book < 350 pages 
# # dataUnderThreshold.average_rating, dataUnderThreshold.num_pages
# averageRatingsUnderThreshold = np.array(dataUnderThreshold.average_rating)
# plt.hist(averageRatingsUnderThreshold, bins, facecolor='r', alpha=0.7, edgecolor='k', linewidth=1)
# plt.title("Average book ratings distribution from data of books containing less than 500 pages")
# plt.xlabel("Average Rating")
# plt.ylabel("Frequency")
# plt.xticks(bins[::1]) 
# plt.xticks(rotation = 45)
# plt.show()


# # both subgroups graphs in the same axes 
# # comparing them on relative frequencies, because comparing them on absolute frequencies wouldn't tell us anything meaningful, instead would probably yield misleading data
# plt.hist(averageRatingsAboveThreshold, bins, facecolor='r', alpha=1, edgecolor='k', linewidth=1, weights=np.ones_like(averageRatingsAboveThreshold) / len(averageRatingsAboveThreshold), label="Books >= 350 pages")
# plt.hist(averageRatingsUnderThreshold, bins, facecolor='b', alpha=.5, edgecolor='k', linewidth=1, weights=np.ones_like(averageRatingsUnderThreshold) / len(averageRatingsUnderThreshold), label="Books < 350 pages")
# plt.xlabel("Average Rating")
# plt.ylabel("Relative Frequency")
# plt.legend()
# plt.xticks(bins[::1]) 
# plt.xticks(rotation = 45)
# plt.show()

df = pd.read_csv("https://docs.google.com/spreadsheets/d/1if5bG_Nld7iQC3FcUBIv2SLT12HQSCqvawvN67iZ-yE/gviz/tq?tqx=out:csv")
import numpy as np
from scipy import stats 

# fetch subgroups from the dataset
# one way to do it is like this:
dataAboveThreshold = df[df['num_pages'] >= 350] # for long books (350 pages or more)
dataUnderThreshold = df[df['num_pages'] < 350] # for short books (less than 350 pages)

# print summary statistics
def additionalStats(sample):
  print("- sample size (n1) =", len(sample))
  print("- mean =", np.mean(sample))
  print("- median =", np.median(sample))
  print("- mode =", stats.mode(sample)[0][0])
  print("- SD =", np.std(sample, ddof=1)) 
  # ^ By default, this function calculates the standard deviation using the population formula, which divides the sum of the 
  # squared differences by the total number of values in the sample. This is not correct for a sample, because it will 
  # underestimate the standard deviation and give a biased result.
  # to correct for this, we should set the ddof argument
  print("- range =", max(sample) - min(sample))


print("Average rating (Long books):")
additionalStats(dataAboveThreshold.average_rating)
print("\n")
print("Average rating (Short books):")
additionalStats(dataUnderThreshold.average_rating)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

df = pd.read_csv("https://docs.google.com/spreadsheets/d/1if5bG_Nld7iQC3FcUBIv2SLT12HQSCqvawvN67iZ-yE/gviz/tq?tqx=out:csv")

dataAboveThreshold = df[df['num_pages'] >= 350]
dataUnderThreshold = df[df['num_pages'] < 350]

bins = [0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 3.75, 4, 4.25, 4.5, 4.75, 5]

# using numpy's np.array() to create an array and plot it in a histogram
# # average ratings for books >= 350
# # dataAboveThreshold.average_rating, dataAboveThreshold.num_pages
averageRatingsAboveThreshold = np.array(dataAboveThreshold.average_rating)
plt.hist(averageRatingsAboveThreshold, bins, facecolor='r', alpha=0.7, edgecolor='k', linewidth=0)
plt.xlabel("Average Rating")
plt.ylabel("Frequency")
plt.xticks(bins[::1]) 
plt.xticks(rotation = 45)
plt.show()

# # average ratings for with book < 350 pages 
# # dataUnderThreshold.average_rating, dataUnderThreshold.num_pages
averageRatingsUnderThreshold = np.array(dataUnderThreshold.average_rating)
plt.hist(averageRatingsUnderThreshold, bins, facecolor='r', alpha=0.7, edgecolor='k', linewidth=0)
plt.xlabel("Average Rating")
plt.ylabel("Frequency")
plt.xticks(bins[::1]) 
plt.xticks(rotation = 45)
plt.show()

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

df = pd.read_csv("https://docs.google.com/spreadsheets/d/1if5bG_Nld7iQC3FcUBIv2SLT12HQSCqvawvN67iZ-yE/gviz/tq?tqx=out:csv")

dataAboveThreshold = df[df['num_pages'] >= 350]
dataUnderThreshold = df[df['num_pages'] < 350]

# the function takes up two lists (samples) and performs a difference of means tests
def difference_of_means_test(data1,data2,tails):
    n1 = len(data1)
    n2 = len(data2)

    x1 = np.mean(data1)
    x2 = np.mean(data2)

    # like in the previous example, np.std() by default calculates the population standard deviation, that is why 
    # we should specify the degrees of freedom (ddof)
    s1 = np.std(data1,ddof=1) 
    s2 = np.std(data2,ddof=1)

    standard_error = np.sqrt(s1**2/n1 + s2**2/n2)
    t_score = np.abs((x2 - x1))/standard_error
    df = min(n1,n2) - 1 # conservative estimate from OpenIntro
    p_value = tails*stats.t.cdf(-t_score,df) # for a 1 or 2-tailed test

    sd_pooled = np.sqrt((s1**2*(n1-1) + s2**2*(n2-1))/(n1+n2-2))
    cohens_d = (x2 - x1)/sd_pooled

    # convert cohens'd to hegde's g
    hedges_g = cohens_d * (1 - (3 / (4 * (n1 + n2) - 9)))


    print('T =', t_score)
    print('p =', p_value)
    print('d =', cohens_d)
    print('g =', hedges_g)


difference_of_means_test(dataUnderThreshold.average_rating, dataAboveThreshold.average_rating,  1)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy.stats as stats

df = pd.read_csv("https://docs.google.com/spreadsheets/d/1if5bG_Nld7iQC3FcUBIv2SLT12HQSCqvawvN67iZ-yE/gviz/tq?tqx=out:csv")

dataAboveThreshold = df[df['num_pages'] >= 350]
dataUnderThreshold = df[df['num_pages'] < 350]

def confidence_interval(data, confidence_level):  
    # Calculate the mean and sample size of the data
    mean = np.mean(data)
    sample_size = len(data)

    # Calculate the standard error
    standard_error = mean / np.sqrt(sample_size)

    # Calculate the degrees of freedom
    df = sample_size - 1

    # Calculate the critical value
    critical_value = stats.t.ppf(1 - (1 - confidence_level) / 2, df)

    # Calculate the lower and upper bounds of the confidence interval
    lower_bound = mean - critical_value * standard_error
    upper_bound = mean + critical_value * standard_error

    # Return the confidence interval as a tuple
    return (lower_bound, upper_bound)

print("95% confidence interval for long books:", confidence_interval(dataAboveThreshold.average_rating, .95))
print("95% confidence interval for short books:", confidence_interval(dataUnderThreshold.average_rating, .95))